{
  "metadata": {
    "name": "autoencoderAnomalyDetector",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# single customer - single feature - long history"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nSN_NUM \u003d \"ABB24511524\""
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nsnr_data_path \u003d \"/user/ZheS//owl_anomally/capacity_records/\"\n#snr_data_path \u003d\"/user/ZheS//owl_anomally////capacity_pplan50127_sliced\"\nfeature_col \u003d \"avg_4gsnr\"\ntime_col \u003d \"hour\"\ncolumns \u003d [\"sn\", time_col, feature_col]\n\ndf_snr_all \u003d spark.read.parquet(snr_data_path)\n\ndf_cap_hour_pd \u003d df_snr_all.select(columns)\\\n                            .filter( col(\"sn\")\u003d\u003d SN_NUM )\\\n                            .orderBy( \"sn\",\"hour\" )\\\n                            .toPandas()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom typing import Optional, Union\nfrom pyod.models.auto_encoder_torch import AutoEncoder\n\nclass AutoencoderAnomalyDetector:\n    def __init__(self, \n                 df: pd.DataFrame, \n                 time_col: str, \n                 feature: str, \n                 window_type: str \u003d \"sliding\",\n                 n_lags: int \u003d 24,\n                 model_params: Optional[dict] \u003d None,\n                 model: Optional[object] \u003d None,\n                 scaler: Union[str, object, None] \u003d \"standard\",\n                 threshold_percentile \u003d 99\n                 ):\n        \"\"\"\n        Initialization.\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n        time_col : str\n        feature : str\n        n_lags : int\n        model_params : dict, optional\n        model : object, optional\n            If provided, this custom model will be used instead of the default autoencoder.\n        scaler : {\u0027standard\u0027, \u0027minmax\u0027, object, None}\n            \u0027standard\u0027 for StandardScaler, \u0027minmax\u0027 for MinMaxScaler,\n            a custom scaler instance (must implement fit_transform), or None.\n        \"\"\"\n        self.df_raw \u003d df.copy()\n        self.time_col \u003d time_col\n        self.feature \u003d feature\n        self.window_type \u003d window_type\n        self.n_lags \u003d n_lags\n        self.model_params \u003d model_params\n        self.external_model \u003d None\n        self.scaler_type \u003d scaler\n        self.scaler \u003d None\n        self.model \u003d None\n        self.threshold_percentile \u003d threshold_percentile\n        \n        self.df \u003d None\n        self.input_data \u003d None\n        self.input_data_scaled \u003d None\n        \n        self.anomaly_scores \u003d None\n        self.threshold_scores \u003d None\n        \n    def _format_time_series(self):\n        df \u003d self.df_raw[[self.time_col, self.feature]].copy()\n        df \u003d df.rename(columns\u003d{self.time_col: \"ds\", self.feature: \"y\"})\n        df[\"unique_id\"] \u003d \"series_1\"\n        return df\n\n    def _segment_time_series(self, series: pd.Series) -\u003e np.ndarray:\n        \"\"\"\n        Generate lagged input sequences from a univariate time series.\n    \n        Parameters\n        ----------\n        series : pd.Series\n            Input univariate time series.\n        window_type : str\n            Type of windowing. Options:\n                - \u0027sliding\u0027: overlapping windows (default)\n                - \u0027block\u0027: non-overlapping segments\n    \n        Returns\n        -------\n        np.ndarray\n            2D array where each row is a lagged input sequence.\n        \"\"\"\n        if self.window_type \u003d\u003d \"sliding\":\n            return np.array([\n                series.iloc[i - self.n_lags:i].values\n                for i in range(self.n_lags, len(series))\n            ])\n        \n        elif self.window_type \u003d\u003d \"block\":\n            num_blocks \u003d len(series) // self.n_lags\n            return np.array([\n                series.iloc[i * self.n_lags : (i + 1) * self.n_lags].values\n                for i in range(num_blocks)\n            ])\n    \n        else:\n            raise ValueError(\"Invalid window_type. Choose \u0027sliding\u0027 or \u0027block\u0027.\")\n\n\n    def _apply_scaler(self, X: np.ndarray) -\u003e np.ndarray:\n        if self.scaler_type is None:\n            return X\n        elif self.scaler_type \u003d\u003d \"standard\":\n            self.scaler \u003d StandardScaler()\n        elif self.scaler_type \u003d\u003d \"minmax\":\n            from sklearn.preprocessing import MinMaxScaler\n            self.scaler \u003d MinMaxScaler()\n        else:\n            self.scaler \u003d self.scaler_type\n        return self.scaler.fit_transform(X)\n\n    def prepare(self):\n        self.df \u003d self._format_time_series()\n        self.input_data \u003d self._segment_time_series(self.df[\"y\"])\n        self.input_data_scaled \u003d self._apply_scaler(self.input_data)\n\n    def _init_model(self):\n        if self.external_model is not None:\n            return self.external_model\n\n        default_params \u003d {\n            \"hidden_neurons\": [self.n_lags, 4, 4, self.n_lags],\n            \"hidden_activation\": \"relu\",\n            \"epochs\": 20,\n            \"batch_norm\": True,\n            \"learning_rate\": 0.001,\n            \"batch_size\": 32,\n            \"dropout_rate\": 0.2,\n        }\n        if self.model_params:\n            default_params.update(self.model_params)\n        return AutoEncoder(**default_params)\n\n    def fit(self, threshold_percentile\u003dNone):\n        if self.input_data_scaled is None:\n            raise ValueError(\"Call prepare() before fit().\")\n        if threshold_percentile is None:\n            threshold_percentile \u003d self.threshold_percentile\n        \n        self.model \u003d self._init_model()\n        self.model.fit(self.input_data_scaled)\n        \n        self.anomaly_scores \u003d self.model.decision_scores_\n        self.threshold_scores \u003d np.percentile(self.anomaly_scores, threshold_percentile)\n        \n    def predict(self, input_series: pd.Series) -\u003e np.ndarray:\n        if self.model is None:\n            raise ValueError(\"Call fit() before predict().\")\n            \n        input_matrix \u003d self._segment_time_series(input_series)\n        \n        if self.scaler:\n            input_matrix \u003d self.scaler.transform(input_matrix)\n        \n        return self.model.decision_function(input_matrix)\n\n    def plot_score_distribution(self, title_id):\n        if self.anomaly_scores is None:\n            raise ValueError(\"Model not trained. Call fit() first.\")\n        plt.figure(figsize\u003d(10, 4))\n        plt.hist(self.anomaly_scores, bins\u003d20, edgecolor\u003d\u0027black\u0027)\n        plt.title(f\"Histogram of Anomaly Scores at {title_id}\")\n        plt.xlabel(\"Anomaly Score\")\n        plt.ylabel(\"Frequency\")\n        plt.grid(True)\n        plt.tight_layout()\n        plt.show()\n\n    def plot_series_with_anomalies(self,title_id):\n        \n        if self.anomaly_scores is None:\n            raise ValueError(\"Model not trained. Call fit() first.\")\n        \n        plt.figure(figsize\u003d(16, 6))\n        plt.plot(self.df[\u0027ds\u0027], self.df[\u0027y\u0027], label\u003d\"Original Time Series\", color\u003d\"blue\")\n        plt.plot(\n            self.df[\u0027ds\u0027][self.n_lags:].values,\n            self.anomaly_scores,\n            color\u003d\"orange\",\n            label\u003d\"Anomaly Score\",\n            linewidth\u003d2\n        )\n        plt.xlabel(\"Time\")\n        plt.ylabel(\"Value / Anomaly Score\")\n        plt.title(f\"Time Series and Anomaly Scores at {title_id}\")\n        plt.legend()\n        plt.grid(True)\n        plt.tight_layout()\n        plt.show()\n\n    def get_anomaly_stats(self):\n        \"\"\"\n        Return anomaly records and scores.\n        \"\"\"\n        \n        if self.anomaly_scores is None:\n            raise ValueError(\"Model not trained. Call fit() first.\")\n    \n    \n        is_outlier \u003d self.anomaly_scores \u003e self.threshold_scores\n    \n        # Create mask for valid rows depending on windowing type\n        if self.window_type \u003d\u003d \"sliding\":\n            base_df \u003d self.df_raw.iloc[self.n_lags:].copy()\n        else:  # \"block\"\n            total_windows \u003d len(self.anomaly_scores)\n            base_df \u003d self.df_raw.iloc[:total_windows * self.n_lags].copy()\n            base_df \u003d base_df.groupby(np.arange(len(base_df)) // self.n_lags).last().reset_index(drop\u003dTrue)\n    \n        base_df[\"anomaly_score\"] \u003d self.anomaly_scores\n        base_df[\"is_outlier\"] \u003d is_outlier\n    \n        anomaly_df \u003d base_df[base_df[\"is_outlier\"]][[\"sn\", self.time_col, self.feature, \"is_outlier\"]]\n    \n        return anomaly_df\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndetector \u003d AutoencoderAnomalyDetector(  df\u003ddf_cap_hour_pd, \n                                        time_col\u003d\"hour\", \n                                        feature\u003d\"avg_4gsnr\",\n                                        window_type \u003d \"sliding\",\n                                        n_lags \u003d 24,\n                                        scaler \u003d None)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndetector.prepare()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndetector.input_data.shape"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndetector.fit()\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndetector.plot_score_distribution(SN_NUM)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndetector.plot_series_with_anomalies(SN_NUM)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nresult \u003d detector.get_anomaly_stats()\n\nresult"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# multiple customer - single feature - short history\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Prepare dataframe"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nsnr_data_path \u003d\"/user/ZheS//owl_anomally////capacity_pplan50127_sliced\"\nfeature_col \u003d \"avg_4gsnr\"\ntime_col \u003d \"hour\"\ncolumns \u003d [\"sn\", time_col, feature_col]\n\ndf_snr_all \u003d spark.read.parquet(snr_data_path)\n\ndf_pandas \u003d df_snr_all.select(\"slice_id\", \"hour\", \"avg_4gsnr\").toPandas()\ndf_pandas.head(3)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_snr_all.select(\"slice_id\").distinct().count()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## building Model"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndetector \u003d AutoencoderAnomalyDetector(  df\u003ddf_pandas, \n                                        time_col\u003d\"hour\", \n                                        feature\u003d\"avg_4gsnr\",\n                                        slice_col \u003d \"slice_id\",\n                                        scaler \u003d None)\n\n# t \u003d detector._build_tensor_from_slices()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndetector.prepare()"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndetector.fit()"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport matplotlib.pyplot as plt\nfrom types import MethodType\n\nimport numpy as np\nfrom types import MethodType\n\ndef predict_score_and_label(self, input_array: np.ndarray, threshold: float \u003d None):\n    \"\"\"\n    Predict anomaly scores and labels for new data.\n\n    Parameters\n    ----------\n    input_array : np.ndarray\n        Array of shape (n_samples, n_lags) matching the model\u0027s input structure.\n    threshold : float, optional\n        Manual threshold for outlier decision. If None, uses trained `self.threshold_scores`.\n\n    Returns\n    -------\n    dict\n        {\n            \"anomaly_scores\": np.ndarray,\n            \"is_outlier\": np.ndarray (bool)\n        }\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model is not trained. Call fit() first.\")\n    \n    # Optionally scale input\n    if self.scaler:\n        input_array \u003d self.scaler.transform(input_array)\n    \n    scores \u003d self.model.decision_function(input_array)\n\n    if threshold is None:\n        if self.threshold_scores is None:\n            raise ValueError(\"Threshold not defined. Either provide it or call fit() first.\")\n        threshold \u003d self.threshold_scores\n\n    is_outlier \u003d scores \u003e threshold\n\n    return {\n        \"anomaly_scores\": scores,\n        \"is_outlier\": is_outlier\n    }\n\n\ndetector.predict_score_and_label \u003d MethodType(predict_score_and_label, detector)\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## get_anomaly_stats"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_outlier \u003d detector.get_anomaly_stats()\ndf_outlier[ df_outlier[\"is_outlier\"]\u003d\u003dTrue ]"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## plot_anomaly_score_distribution"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndetector.plot_anomaly_score_distribution()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## predict_score_and_label"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport numpy as np\n\ndf_one_slice \u003d df_pandas[ df_pandas[\"slice_id\"]\u003d\u003d\"ABB24700945_0\" ]\n\n# Assume df_one_slice is your 170-row DataFrame\ninput_array \u003d df_one_slice[\"avg_4gsnr\"].values.reshape(1, -1)\nresult \u003d detector.predict_score_and_label(input_array)\nresult"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## plot_series_btw_anomaly_and_normal"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndetector.plot_time_series_by_category \u003d MethodType(plot_time_series_by_category, detector)"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Plot 100 normal samples\ndetector.plot_time_series_by_category(category\u003d\"normal\", n_samples\u003d100)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Plot 100 abnormal samples\ndetector.plot_time_series_by_category(category\u003d\"abnormal\", n_samples\u003d100)"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\n\n# Plot samples with scores in a specific range\ndetector.plot_time_series_by_category(category\u003d(0, 6), n_samples\u003d10)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndetector.plot_sample_series_by_anomaly()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## plot_mean_and_spread"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndetector.plot_mean_and_spread()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## predict_and_compare_with_normal"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom matplotlib import cm\ndef predict_and_compare_with_normal(self,\n                                    input_array: np.ndarray,\n                                    title_id: str \u003d \"\",\n                                    n_normal_samples: int \u003d 100,\n                                    normal_score_range: Optional[Tuple[float, float]] \u003d None):\n    \"\"\"\n    Predict anomaly score of an input time series and compare it with normal training samples.\n\n    Parameters\n    ----------\n    input_array : np.ndarray\n        Shape (1, time_steps). The new time series to evaluate.\n    title_id : str\n        Identifier for labeling the input series on the plot.\n    n_normal_samples : int\n        Number of normal training series to plot for comparison.\n    normal_score_range : tuple, optional\n        If provided, defines (min_score, max_score) range to select normal samples\n        from the training data. Overrides threshold-based selection.\n    \"\"\"\n    if self.model is None or self.anomaly_scores is None:\n        raise ValueError(\"Model not trained. Call fit() first.\")\n\n    # Ensure 2D shape for input\n    if input_array.ndim \u003d\u003d 1:\n        input_array \u003d input_array.reshape(1, -1)\n\n    if self.scaler:\n        input_array_scaled \u003d self.scaler.transform(input_array)\n    else:\n        input_array_scaled \u003d input_array\n\n    scores \u003d self.model.decision_function(input_array_scaled)\n    labels \u003d scores \u003e self.threshold_scores\n\n    # Select normal samples from training\n    scores_all \u003d self.anomaly_scores\n    if normal_score_range is not None:\n        min_score, max_score \u003d normal_score_range\n        normal_idx \u003d np.where((scores_all \u003e\u003d min_score) \u0026 (scores_all \u003c\u003d max_score))[0]\n    else:\n        normal_idx \u003d np.where(scores_all \u003c\u003d self.threshold_scores)[0]\n\n    sample_n_normal \u003d min(n_normal_samples, len(normal_idx))\n    if sample_n_normal \u003d\u003d 0:\n        raise ValueError(\"No normal samples found in the specified range.\")\n\n    selected_idx \u003d np.random.choice(normal_idx, size\u003dsample_n_normal, replace\u003dFalse)\n    normal_samples \u003d self.input_tensor[selected_idx, :, 0]\n\n    # Plot\n    plt.figure(figsize\u003d(12, 5))\n    cmap \u003d cm.get_cmap(\u0027viridis\u0027, sample_n_normal)\n    for i, series in enumerate(normal_samples):\n        plt.plot(series, color\u003dcmap(i), alpha\u003d0.5)\n\n    # Plot the input series\n    input_series \u003d input_array[0]\n    label \u003d \u0027Abnormal\u0027 if labels[0] else \u0027Normal\u0027\n    color \u003d \u0027red\u0027 if labels[0] else \u0027darkgreen\u0027\n    plt.plot(input_series, linewidth\u003d2.5, color\u003dcolor, label\u003df\"Input Series\")\n\n    # Add annotation in bottom-right\n    annotation \u003d (f\"Score: {scores[0]:.4f}\\n\"\n                  f\"Thresh: {self.threshold_scores:.4f}\\n\"\n                  f\"Outlier: {label}\")\n    plt.annotate(annotation,\n                 xy\u003d(1.0, 0.0), xycoords\u003d\u0027axes fraction\u0027,\n                 xytext\u003d(-10, 10), textcoords\u003d\u0027offset points\u0027,\n                 ha\u003d\u0027right\u0027, va\u003d\u0027bottom\u0027,\n                 fontsize\u003d9,\n                 bbox\u003ddict(boxstyle\u003d\"round\", fc\u003d\"white\", ec\u003dcolor, alpha\u003d0.8))\n\n    plt.title(f\"Comparison of Input Series with Normal Samples\")\n    plt.xlabel(\"Time Index\")\n    plt.ylabel(self.feature)\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()\n\n\n\ndetector.predict_and_compare_with_normal \u003d MethodType(predict_and_compare_with_normal, detector)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport numpy as np\n\ndf_one_slice \u003d df_pandas[ df_pandas[\"slice_id\"]\u003d\u003d\"ABB24700945_0\" ]\n\n# Assume df_one_slice is your 170-row DataFrame\ninput_array \u003d df_one_slice[\"avg_4gsnr\"].values.reshape(1, -1)\ndetector.predict_and_compare_with_normal(input_array,\n                                        n_normal_samples \u003d 10,\n                                        title_id \u003d \"ABB24700945_0\"\n                                            )\n"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndetector.predict_and_compare_with_normal(input_array,\n                                        n_normal_samples \u003d 10,\n                                        title_id \u003d \"ABB24700945_0\",\n                                        normal_score_range \u003d [0,5]\n                                            )"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Define Class"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nclass AutoencoderAnomalyDetector:\n    def __init__(self,\n                 df: pd.DataFrame,\n                 time_col: str,\n                 feature: str,\n                 slice_col: str \u003d \"slice_id\",\n                 model_params: Optional[dict] \u003d None,\n                 external_model: Optional[object] \u003d None,\n                 scaler: Union[str, object, None] \u003d \"None\",\n                 threshold_percentile: float \u003d 99):\n        self.df_raw \u003d df.copy()\n        self.time_col \u003d time_col\n        self.feature \u003d feature\n        self.slice_col \u003d slice_col\n        self.model_params \u003d model_params\n        self.external_model \u003d external_model\n        self.scaler_type \u003d scaler\n        self.scaler \u003d None\n        self.model \u003d None\n        self.threshold_percentile \u003d threshold_percentile\n\n        self.input_tensor \u003d None\n        self.input_tensor_scaled \u003d None\n        self.anomaly_scores \u003d None\n        self.threshold_scores \u003d None\n\n    def _build_tensor_from_slices(self):\n        grouped \u003d self.df_raw.groupby(self.slice_col)\n        tensors \u003d []\n\n        for _, group in grouped:\n            series \u003d group.sort_values(by\u003dself.time_col)[self.feature].values\n            tensors.append(series)\n\n        tensor_3d \u003d np.stack(tensors)[:, :, np.newaxis]  # shape: (n_samples, n_timesteps, 1)\n        return tensor_3d\n\n    def _apply_scaler(self, X: np.ndarray) -\u003e np.ndarray:\n        if self.scaler_type is None:\n            return X\n        flat_X \u003d X.reshape(-1, X.shape[-1])  # flatten across time axis\n        if self.scaler_type \u003d\u003d \"standard\":\n            self.scaler \u003d StandardScaler()\n        elif self.scaler_type \u003d\u003d \"minmax\":\n            from sklearn.preprocessing import MinMaxScaler\n            self.scaler \u003d MinMaxScaler()\n        else:\n            self.scaler \u003d self.scaler_type\n        scaled_flat \u003d self.scaler.fit_transform(flat_X)\n        return scaled_flat.reshape(X.shape)\n\n    def prepare(self):\n        tensor \u003d self._build_tensor_from_slices()\n        self.input_tensor \u003d tensor\n        self.input_tensor_scaled \u003d self._apply_scaler(tensor)\n\n    def _init_model(self):\n        if self.external_model:\n            return self.external_model\n        default_params \u003d {\n            \"hidden_neurons\": [self.input_tensor.shape[1], 32, 32, self.input_tensor.shape[1]],\n            \"hidden_activation\": \"relu\",\n            \"epochs\": 20,\n            \"batch_norm\": True,\n            \"learning_rate\": 0.001,\n            \"batch_size\": 32,\n            \"dropout_rate\": 0.2,\n        }\n        if self.model_params:\n            default_params.update(self.model_params)\n        return AutoEncoder(**default_params)\n\n    def fit(self, threshold_percentile\u003dNone):\n        if self.input_tensor_scaled is None:\n            raise ValueError(\"Call prepare() before fit().\")\n        if threshold_percentile is None:\n            threshold_percentile \u003d self.threshold_percentile\n\n        n_samples \u003d self.input_tensor_scaled.shape[0]\n        X \u003d self.input_tensor_scaled.reshape(n_samples, -1)  # flatten to 2D for sklearn-compatible model\n        self.model \u003d self._init_model()\n        self.model.fit(X)\n\n        self.anomaly_scores \u003d self.model.decision_scores_\n        self.threshold_scores \u003d np.percentile(self.anomaly_scores, threshold_percentile)\n\n\n    def get_anomaly_stats(self):\n        \"\"\"\n        Return anomaly scores and labels per slice (1 row per slice_id).\n\n        Returns\n        -------\n        pd.DataFrame\n            A DataFrame with columns [\u0027sn\u0027, slice_col, \u0027anomaly_score\u0027, \u0027is_outlier\u0027]\n        \"\"\"\n        if self.anomaly_scores is None:\n            raise ValueError(\"Call fit() first.\")\n\n        is_outlier \u003d self.anomaly_scores \u003e self.threshold_scores\n\n        unique_slices \u003d self.df_raw[[self.slice_col]].drop_duplicates().reset_index(drop\u003dTrue)\n        result_df \u003d unique_slices.copy()\n        result_df[\"anomaly_score\"] \u003d self.anomaly_scores\n        result_df[\"is_outlier\"] \u003d is_outlier\n        result_df[\"sn\"] \u003d result_df[self.slice_col].apply(lambda x: str(x).split(\"_\")[0])\n\n        return result_df[[\"sn\", self.slice_col, \"anomaly_score\", \"is_outlier\"]]\n\n\n    def predict_score_and_label(self, input_array: np.ndarray, threshold: float \u003d None):\n        \"\"\"\n        Predict anomaly scores and labels for new data.\n\n        Parameters\n        ----------\n        input_array : np.ndarray\n            Array of shape (n_samples, n_lags) matching the model\u0027s input structure.\n        threshold : float, optional\n            Manual threshold for outlier decision. If None, uses trained `self.threshold_scores`.\n\n        Returns\n        -------\n        dict\n            {\n                \"anomaly_scores\": np.ndarray,\n                \"is_outlier\": np.ndarray (bool)\n            }\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model is not trained. Call fit() first.\")\n        \n        # Optionally scale input\n        if self.scaler:\n            input_array \u003d self.scaler.transform(input_array)\n        \n        scores \u003d self.model.decision_function(input_array)\n\n        if threshold is None:\n            if self.threshold_scores is None:\n                raise ValueError(\"Threshold not defined. Either provide it or call fit() first.\")\n            threshold \u003d self.threshold_scores\n\n        is_outlier \u003d scores \u003e threshold\n\n        return {\n            \"anomaly_scores\": scores,\n            \"is_outlier\": is_outlier\n        }\n\n    def plot_anomaly_score_distribution(self, bins\u003d30, sample_size\u003d10000, random_state\u003d42):\n        \"\"\"\n        Plot the distribution of anomaly scores (with optional downsampling).\n\n        Parameters\n        ----------\n        bins : int\n            Number of histogram bins (default\u003d30).\n        sample_size : int\n            Number of scores to sample for plotting. If the total number of scores is less than this, use all.\n        random_state : int\n            Seed for reproducible sampling.\n        \"\"\"\n        if self.anomaly_scores is None:\n            raise ValueError(\"Call fit() before plotting anomaly scores.\")\n        \n        scores \u003d self.anomaly_scores\n        if len(scores) \u003e sample_size:\n            np.random.seed(random_state)\n            scores \u003d np.random.choice(scores, size\u003dsample_size, replace\u003dFalse)\n\n        plt.figure(figsize\u003d(10, 5))\n        plt.hist(scores, bins\u003dbins, edgecolor\u003d\u0027black\u0027, alpha\u003d0.8)\n        plt.axvline(self.threshold_scores, color\u003d\u0027red\u0027, linestyle\u003d\u0027--\u0027, label\u003df\u0027Threshold \u003d {self.threshold_scores:.4f}\u0027)\n        plt.title(f\"Anomaly Score Distribution (n\u003d{len(scores)} sample{\u0027s\u0027 if len(scores) \u003e 1 else \u0027\u0027})\")\n        plt.xlabel(\"Anomaly Score\")\n        plt.ylabel(\"Frequency\")\n        plt.legend()\n        plt.grid(True)\n        plt.tight_layout()\n        plt.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndims \u003d [2, 10, 50, 100, 200, 500]\navg_ratios \u003d []\n\nfor d in dims:\n    points \u003d np.random.randn(1000, d)\n    ref \u003d points[0]\n    dists \u003d np.linalg.norm(points - ref, axis\u003d1)\n    ratio \u003d (np.max(dists) - np.min(dists)) / np.mean(dists)\n    avg_ratios.append(ratio)\n\nplt.figure(figsize\u003d(8, 5))\nplt.plot(dims, avg_ratios, marker\u003d\u0027o\u0027)\nplt.title(\"Distance Contrast Vanishes in High Dimensions\")\nplt.xlabel(\"Dimensionality (d)\")\nplt.ylabel(\"(max - min) / mean distance\")\nplt.grid(True)\nplt.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nerrors \u003d np.random.rand(168, 50)\n\nplt.figure(figsize\u003d(12, 6))\nsns.heatmap(errors, cmap\u003d\"YlOrRd\", cbar_kws\u003d{\u0027label\u0027: \u0027Numerical Value\u0027})\nplt.xlabel(\"Feature Index\")\nplt.ylabel(\"Hour of Week\")\nplt.title(\"Reconstruction Error Heatmap (1 Customer, 1 Week)\")\nplt.tight_layout()\nplt.show()\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}